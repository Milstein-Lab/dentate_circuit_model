optimization_title: dentate_add_I_I_no_learning

param_names: # params to optimize, format as: parameter;postsynaptic;presynaptic
    - mean_weight;Output;Input
    - mean_weight;FF_Inh;Input
    - mean_weight;Output;FF_Inh
    - mean_weight;FF_Inh;FF_Inh
    - mean_weight;FB_Exc;Output
    - mean_weight;FB_Exc;FB_Exc
    - mean_weight;FB_Exc;FB_Inh
    - mean_weight;FB_Inh;FB_Exc
    - mean_weight;FB_Inh;FB_Inh
    - mean_weight;Output;FB_Inh
    - learning_rate

bounds:
    mean_weight;Output;Input: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FF_Inh;Input: !!python/tuple
        - 0.01
        - 2.
    mean_weight;Output;FF_Inh: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FF_Inh;FF_Inh: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FB_Exc;Output: !!python/tuple
        - 0.001
        - 2.
    mean_weight;FB_Exc;FB_Exc: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FB_Exc;FB_Inh: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FB_Inh;FB_Exc: !!python/tuple
        - 0.01
        - 2.
    mean_weight;FB_Inh;FB_Inh: !!python/tuple
        - 0.01
        - 2.
    mean_weight;Output;FB_Inh: !!python/tuple
        - 0.01
        - 2.
    learning_rate:  !!python/tuple
        - 0.005
        - 0.1

default_params:
    learning_rate: 0.

feature_names:
    - sparsity
    - similarity
    - selectivity
    - fraction_active_patterns
    - fraction_active_units

stages:
    - source: optimize_dynamic_model
      get_args_static: get_weight_seeds
      compute_features: compute_features_multiple_instances
      filter_features: filter_features_multiple_instances
      get_objectives: get_objectives_multiple_instances

objective_names: #loss/error values to minimize
    - sparsity_loss
    - discriminability_loss
    - selectivity_loss
    - fraction_active_patterns_loss
    - fraction_active_units_loss

target_val:
    sparsity: 1. # number of nonzero units for each pattern
    similarity: 0. # target cosine similarity for each pair of patterns
    selectivity: 1. # number of nonzero patters for each unit
    fraction_active_patterns: 1. # fraction of patterns with any nonzero output activity
    fraction_active_units: 1. # fraction of output units with any nonzero activity

target_range: #scaling factor to determine sensitivity of each objective
    sparsity: 0.1
    similarity: 0.1
    selectivity: 0.1
    fraction_active_patterns: 0.001
    fraction_active_units: 0.001

x0: #initial parameters
    mean_weight;Output;Input: 0.5
    mean_weight;FF_Inh;Input: 0.5
    mean_weight;Output;FF_Inh: 0.5
    mean_weight;FF_Inh;FF_Inh: 0.5
    mean_weight;FB_Exc;Output: 0.5
    mean_weight;FB_Exc;FB_Exc: 0.5
    mean_weight;FB_Exc;FB_Inh: 0.5
    mean_weight;FB_Inh;FB_Exc: 0.5
    mean_weight;FB_Inh;FB_Inh: 0.5
    mean_weight;Output;FB_Inh: 0.5
    learning_rate: 0.

param_gen: PopulationAnnealing

kwargs: #All these will become will become dictionaries in Context() once nested.optimize is called
    description: add_I_I_no_learning
    init_weight_seed: 1234
    num_instances: 5
    duration: 0.35 #sec
    dt: 0.01 #sec
    train_epochs: 0 # 10
    train_seed: 0
    time_point: !!python/tuple
        - 0.15
        - 0.35
    plot_patterns: False
    fraction_active_patterns_threshold:
        Output: 0.9
        default: 0.6
    fraction_active_units_threshold:
        Output: 0.9
        default: 0.8

    num_units_dict:
        Input: 7
        Output: 128
        FF_Inh: 7
        FB_Inh: 7
        FB_Exc: 7

    activation_function_dict:
        Output:
            Name: rectified_linear_activation
            Arguments:
                slope: 0.2
                threshold: 10.
        FF_Inh:
            Name: rectified_linear_activation
            Arguments:
                slope: 0.2
                threshold: 10.
        FB_Inh:
            Name: rectified_linear_activation
            Arguments:
                slope: 0.2
                threshold: 10.
        FB_Exc:
            Name: rectified_linear_activation
            Arguments:
                slope: 0.2
                threshold: 10.

    weight_config_dict:
        Output:
            Input:
                dist_type: log-normal
                mean_magnitude: 0.47505384640581827
                connection_type: exc
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FF_Inh:
                dist_type: uniform
                mean_magnitude: 0.5192539618097884
                connection_type: inh
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FB_Inh:
                dist_type: uniform
                mean_magnitude: 1.5684222744330798
                connection_type: inh
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
        FF_Inh:
            Input:
                dist_type: uniform
                mean_magnitude: 0.2496283351961201
                connection_type: exc
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FF_Inh:
                dist_type: uniform
                mean_magnitude: 0.5192539618097884
                connection_type: inh
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
        FB_Exc:
            Output:
                dist_type: log-normal
                mean_magnitude: 0.016869824127259937
                connection_type: exc
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FB_Exc:
                dist_type: log-normal
                mean_magnitude: 0.6720853682836645
                connection_type: exc
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FB_Inh:
                dist_type: uniform
                mean_magnitude: 1.5684222744330798
                connection_type: inh
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
        FB_Inh:
            FB_Exc:
                dist_type: uniform
                mean_magnitude: 1.0055371025973854
                connection_type: exc
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01
            FB_Inh:
                dist_type: uniform
                mean_magnitude: 1.5684222744330798
                connection_type: inh
                learning_rule: Hebb_weight_norm
                learning_rule_params:
                    learning_rate: 0.01

    cell_tau_dict:
        Output: 0.05
        FF_Inh: 0.02
        FB_Inh: 0.02
        FB_Exc: 0.05

    synapse_tau_dict:
        Output:
            Input:
                rise: 0.001
                decay: 0.01
            FF_Inh:
                rise: 0.001
                decay: 0.02
            FB_Inh:
                rise: 0.001
                decay: 0.02
        FF_Inh:
            Input:
                rise: 0.001
                decay: 0.01
            FF_Inh:
                rise: 0.001
                decay: 0.02
        FB_Exc:
            Output:
                rise: 0.001
                decay: 0.01
            FB_Exc:
                rise: 0.001
                decay: 0.01
            FB_Inh:
                rise: 0.001
                decay: 0.02
        FB_Inh:
            FB_Exc:
                rise: 0.001
                decay: 0.01
            FB_Inh:
                rise: 0.001
                decay: 0.02

    synaptic_reversal_dict:
        exc: 60.
        inh: -10.